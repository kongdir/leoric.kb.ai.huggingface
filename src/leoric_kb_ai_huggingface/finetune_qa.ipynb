{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4809987e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from datasets import load_from_disk\n",
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering, Trainer, TrainingArguments\n",
    "\n",
    "from sklearn.metrics import f1_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "89ea135a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetInfo(description='', citation='', homepage='', license='', features={'passage_answer_candidates': {'plaintext_start_byte': List(Value('int32')), 'plaintext_end_byte': List(Value('int32'))}, 'question_text': Value('string'), 'document_title': Value('string'), 'language': Value('string'), 'annotations': {'passage_answer_candidate_index': List(Value('int32')), 'minimal_answers_start_byte': List(Value('int32')), 'minimal_answers_end_byte': List(Value('int32')), 'yes_no_answer': List(Value('string'))}, 'document_plaintext': Value('string'), 'document_url': Value('string')}, post_processed=None, supervised_keys=None, builder_name='parquet', dataset_name='tydiqa', config_name='primary_task', version=0.0.0, splits={'train': SplitInfo(name='train', num_bytes=5552349645, num_examples=166916, shard_lengths=[15910, 15910, 15910, 15910, 15910, 15910, 14910, 15819, 14909, 14909, 10909], dataset_name='tydiqa'), 'validation': SplitInfo(name='validation', num_bytes=484565021, num_examples=18670, shard_lengths=None, dataset_name='tydiqa')}, download_checksums={'hf://datasets/google-research-datasets/tydiqa@da78f23f9119363459acbaf46bf89426ff26c259/primary_task/train-00000-of-00012.parquet': {'num_bytes': 218765187, 'checksum': None}, 'hf://datasets/google-research-datasets/tydiqa@da78f23f9119363459acbaf46bf89426ff26c259/primary_task/train-00001-of-00012.parquet': {'num_bytes': 224918293, 'checksum': None}, 'hf://datasets/google-research-datasets/tydiqa@da78f23f9119363459acbaf46bf89426ff26c259/primary_task/train-00002-of-00012.parquet': {'num_bytes': 224137195, 'checksum': None}, 'hf://datasets/google-research-datasets/tydiqa@da78f23f9119363459acbaf46bf89426ff26c259/primary_task/train-00003-of-00012.parquet': {'num_bytes': 226849712, 'checksum': None}, 'hf://datasets/google-research-datasets/tydiqa@da78f23f9119363459acbaf46bf89426ff26c259/primary_task/train-00004-of-00012.parquet': {'num_bytes': 226002108, 'checksum': None}, 'hf://datasets/google-research-datasets/tydiqa@da78f23f9119363459acbaf46bf89426ff26c259/primary_task/train-00005-of-00012.parquet': {'num_bytes': 224119388, 'checksum': None}, 'hf://datasets/google-research-datasets/tydiqa@da78f23f9119363459acbaf46bf89426ff26c259/primary_task/train-00006-of-00012.parquet': {'num_bytes': 224002879, 'checksum': None}, 'hf://datasets/google-research-datasets/tydiqa@da78f23f9119363459acbaf46bf89426ff26c259/primary_task/train-00007-of-00012.parquet': {'num_bytes': 225694370, 'checksum': None}, 'hf://datasets/google-research-datasets/tydiqa@da78f23f9119363459acbaf46bf89426ff26c259/primary_task/train-00008-of-00012.parquet': {'num_bytes': 220991562, 'checksum': None}, 'hf://datasets/google-research-datasets/tydiqa@da78f23f9119363459acbaf46bf89426ff26c259/primary_task/train-00009-of-00012.parquet': {'num_bytes': 222688638, 'checksum': None}, 'hf://datasets/google-research-datasets/tydiqa@da78f23f9119363459acbaf46bf89426ff26c259/primary_task/train-00010-of-00012.parquet': {'num_bytes': 225095519, 'checksum': None}, 'hf://datasets/google-research-datasets/tydiqa@da78f23f9119363459acbaf46bf89426ff26c259/primary_task/train-00011-of-00012.parquet': {'num_bytes': 216812403, 'checksum': None}, 'hf://datasets/google-research-datasets/tydiqa@da78f23f9119363459acbaf46bf89426ff26c259/primary_task/validation-00000-of-00001.parquet': {'num_bytes': 232035124, 'checksum': None}}, download_size=2912112378, post_processing_size=None, dataset_size=6036914666, size_in_bytes=8949027044)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inspect a dataset without downloading it\n",
    "from datasets import load_dataset_builder\n",
    "ds_builder = load_dataset_builder(\"google-research-datasets/tydiqa\", \"primary_task\")\n",
    "ds_builder.info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e9b3a635",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['passage_answer_candidates', 'question_text', 'document_title', 'language', 'annotations', 'document_plaintext', 'document_url'],\n",
       "        num_rows: 166916\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['passage_answer_candidates', 'question_text', 'document_title', 'language', 'annotations', 'document_plaintext', 'document_url'],\n",
       "        num_rows: 18670\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load dataset\n",
    "from datasets import load_dataset\n",
    "tydiqa_data = load_dataset(\"google-research-datasets/tydiqa\", 'primary_task')\n",
    "tydiqa_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5301ee99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Mit√§ on altruismi?\n",
      "\n",
      "Context (truncated): \n",
      "\n",
      "\n",
      "Altruismi ([1],  ‚Äùtoinen‚Äù[2]) tarkoittaa ep√§itsek√§st√§ ja pyyteet√∂nt√§[3] [4] toimintaa, jossa toisen hyv√§ asetetaan oman edun edelle.[5] Altruismin vastakohta on egoismi.[6] Termin esitti ranskalainen filosofi Auguste Comte vuonna 1851, jolloin h√§n m√§√§ritteli altruismin uhrautumiseksi muiden eduksi.[1]\n",
      "Etiikka\n",
      "Etiikassa altruismi on oppi, jonka mukaan teon moraalisuus m√§√§ritell√§√§n sen mukaan, tuottaako se hyv√§√§ muille. Altruismi on egoismin vastakohta. Altruismi ei sin√§ns√§ m√§√§rittele sit√§, millainen teko  ...\n",
      "\n",
      "Answer: tsek√§st√§ ja pyyteet√∂nt√§[3] [4] toimintaa, jossa toisen hyv√§ asetetaan oman edun edelle.[5] Altru\n"
     ]
    }
   ],
   "source": [
    "idx = 30\n",
    "\n",
    "# start index\n",
    "start_index = tydiqa_data['train'][idx]['annotations']['minimal_answers_start_byte'][0]\n",
    "\n",
    "# end index\n",
    "end_index = tydiqa_data['train'][idx]['annotations']['minimal_answers_end_byte'][0]\n",
    "\n",
    "print(f\"Question: {tydiqa_data['train'][idx]['question_text']}\")\n",
    "print(f\"\\nContext (truncated): {tydiqa_data['train'][idx]['document_plaintext'][0:512]} ...\")\n",
    "print(f\"\\nAnswer: {tydiqa_data['train'][idx]['document_plaintext'][start_index:end_index]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4810b52b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['passage_answer_candidates.plaintext_start_byte', 'passage_answer_candidates.plaintext_end_byte', 'question_text', 'document_title', 'language', 'annotations.passage_answer_candidate_index', 'annotations.minimal_answers_start_byte', 'annotations.minimal_answers_end_byte', 'annotations.yes_no_answer', 'document_plaintext', 'document_url'],\n",
       "    num_rows: 166916\n",
       "})"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#¬†Flattening the datasets\n",
    "flattened_train_data = tydiqa_data['train'].flatten()\n",
    "flattened_test_data =  tydiqa_data['validation'].flatten()\n",
    "flattened_train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "cdb5953e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting a subset of the train dataset and test dataset\n",
    "flattened_train_data = flattened_train_data.select(range(3000))\n",
    "flattened_test_data = flattened_test_data.select(range(3000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e2b1699c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the AutoTokenizer from the transformers library\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-cased-distilled-squad\")\n",
    "\n",
    "# Define max length of sequences in the tokenizer\n",
    "tokenizer.model_max_length = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1b1d6bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given the characteristics of the dataset and the question-answering task, you will need to add some steps to pre-process the data after the tokenization:\n",
    "\n",
    "# When there is no answer to a question given a context, you will use the CLS token, a unique token used to represent the start of the sequence.\n",
    "# Tokenizers can split a given string into substrings, resulting in a subtoken for each substring, creating misalignment between the list of dataset tags and the labels generated by the tokenizer. Therefore, you will need to align the start and end indices with the tokens associated with the target answer word.\n",
    "# Finally, a tokenizer can truncate a very long sequence. So, if the start/end position of an answer is None, you will assume that it was truncated and assign the maximum length of the tokenizer to those positions.\n",
    "def process_samples(sample):\n",
    "    # two-sentence (or two-segment) encoding used by models like BERT\n",
    "    try:\n",
    "        tokenized_data = tokenizer(sample['question_text'], sample['document_plaintext'], truncation=True, padding=\"max_length\")\n",
    "    except Exception as e:\n",
    "        # Optionally, log the error and sample for debugging\n",
    "        print(f\"Skipping sample due to error: {e}\")\n",
    "        return None\n",
    "\n",
    "    # [CLS] document tokens ... [SEP] question tokens ... [SEP]\n",
    "    input_ids = tokenized_data[\"input_ids\"]\n",
    "\n",
    "    # We will label impossible answers with the index of the CLS token.\n",
    "    # Should be 0\n",
    "    cls_index = input_ids.index(tokenizer.cls_token_id)\n",
    "\n",
    "    # If no answers are given, set the cls_index as answer.\n",
    "    if sample[\"annotations.minimal_answers_start_byte\"][0] == -1:\n",
    "        start_position = cls_index\n",
    "        end_position = cls_index\n",
    "    else:\n",
    "        # Start/end character index of the answer in the text.\n",
    "        gold_text = sample[\"document_plaintext\"][sample['annotations.minimal_answers_start_byte'][0]:sample['annotations.minimal_answers_end_byte'][0]]\n",
    "        start_char = sample[\"annotations.minimal_answers_start_byte\"][0]\n",
    "        end_char = sample['annotations.minimal_answers_end_byte'][0] #start_char + len(gold_text)\n",
    "\n",
    "        # sometimes answers are off by a character or two ‚Äì fix this\n",
    "        if sample['document_plaintext'][start_char-1:end_char-1] == gold_text:\n",
    "            start_char = start_char - 1\n",
    "            end_char = end_char - 1     # When the gold label is off by one character\n",
    "        elif sample['document_plaintext'][start_char-2:end_char-2] == gold_text:\n",
    "            start_char = start_char - 2\n",
    "            end_char = end_char - 2     # When the gold label is off by two characters\n",
    "\n",
    "        # char_to_token(char_index) map a character position in the original text to the corresponding token index in the encoded sequence\n",
    "        start_token = tokenized_data.char_to_token(start_char)\n",
    "        end_token = tokenized_data.char_to_token(end_char - 1)\n",
    "\n",
    "        # if start position is None, the answer passage has been truncated\n",
    "        # Get None if the character is inside a part that gets removed or not tokenized (e.g., whitespace or special characters depending on tokenizer)\n",
    "        if start_token is None:\n",
    "            start_token = tokenizer.model_max_length\n",
    "        if end_token is None:\n",
    "            end_token = tokenizer.model_max_length\n",
    "\n",
    "        start_position = start_token\n",
    "        end_position = end_token\n",
    "\n",
    "    return {'input_ids': tokenized_data['input_ids'],\n",
    "          'attention_mask': tokenized_data['attention_mask'],\n",
    "          'start_positions': start_position,\n",
    "          'end_positions': end_position}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a54138b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['passage_answer_candidates.plaintext_start_byte', 'passage_answer_candidates.plaintext_end_byte', 'question_text', 'document_title', 'language', 'annotations.passage_answer_candidate_index', 'annotations.minimal_answers_start_byte', 'annotations.minimal_answers_end_byte', 'annotations.yes_no_answer', 'document_plaintext', 'document_url', 'input_ids', 'attention_mask', 'start_positions', 'end_positions'],\n",
       "    num_rows: 3000\n",
       "})"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenizing and processing the flattened dataset\n",
    "# Apply a function to all the examples in the table (individually or in batches) and update the table by adding new columns.\n",
    "# If your function returns a column that already exists, then it overwrites it.\n",
    "processed_train_data = flattened_train_data.map(process_samples)\n",
    "processed_test_data = flattened_test_data.map(process_samples)\n",
    "processed_train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "53ead9f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DistilBertForQuestionAnswering(\n",
      "  (distilbert): DistilBertModel(\n",
      "    (embeddings): Embeddings(\n",
      "      (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (transformer): Transformer(\n",
      "      (layer): ModuleList(\n",
      "        (0-5): 6 x TransformerBlock(\n",
      "          (attention): DistilBertSdpaAttention(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (ffn): FFN(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (activation): GELUActivation()\n",
      "          )\n",
      "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (qa_outputs): Linear(in_features=768, out_features=2, bias=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Import the AutoModelForQuestionAnswering for the pre-trained model. You will only fine tune the head of the model\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(\"distilbert-base-cased-distilled-squad\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "208b73bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_return = ['input_ids','attention_mask', 'start_positions', 'end_positions']\n",
    "\n",
    "# Set the format of the datasets to PyTorch tensors\n",
    "processed_train_data.set_format(type='torch', columns=columns_to_return)\n",
    "processed_test_data.set_format(type='torch', columns=columns_to_return)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "535353ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_f1_metrics(pred):\n",
    "    # for extractive QA, the pred.predictions is a tuple of (start_logits, end_logits)\n",
    "    # start_logits shape: (sample size, sequence_length)\n",
    "    # end_logits shape: (sample size, sequence_length) \n",
    "    # the pred.label_ids is also a tuple of (start_positions, end_positions)\n",
    "    # start_positions shape: (sample size, )\n",
    "    # end_positions shape: (sample size, )\n",
    "    start_logits, end_logits = pred.predictions\n",
    "    start_labels, end_labels = pred.label_ids\n",
    "\n",
    "    # ÊâìÂç∞Á±ªÂûãÂíåÂΩ¢Áä∂ÔºåÂ∏ÆÂä©Ë∞ÉËØï\n",
    "    print(f\"È¢ÑÊµãÂÄºÁ±ªÂûã: {type(start_logits)}, ÂΩ¢Áä∂: {start_logits.shape}\") # (3000, 512)\n",
    "    print(f\"ÁúüÂÆûÊ†áÁ≠æÁ±ªÂûã: {type(start_labels)}ÔºåÂΩ¢Áä∂: {start_labels.shape}\") # (3000, 512)\n",
    "\n",
    "    start_preds = start_logits.argmax(-1)\n",
    "    end_preds = end_logits.argmax(-1)\n",
    "\n",
    "    f1_start = f1_score(start_labels, start_preds, average='macro')\n",
    "    f1_end = f1_score(end_labels, end_preds, average='macro')\n",
    "\n",
    "    return {\n",
    "        'f1_start': f1_start,\n",
    "        'f1_end': f1_end,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "757dfe59",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/l0k00au/Projects/Leoric/AI/leoric.kb.ai.huggingface/.venv/lib/python3.14/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='564' max='564' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [564/564 23:23, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.493200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.159100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.251100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.143000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.139900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.129100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.133900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.136500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.086700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.121500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.069600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=564, training_loss=0.1720068002423496, metrics={'train_runtime': 1411.5479, 'train_samples_per_second': 6.376, 'train_steps_per_second': 0.4, 'total_flos': 1175877900288000.0, 'train_loss': 0.1720068002423496, 'epoch': 3.0})"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training hyperparameters\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='model_results',     # output directory\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=3,              # total number of training epochs\n",
    "    per_device_train_batch_size=16,   # batch size per device during training\n",
    "    per_device_eval_batch_size=8,    # batch size for evaluation\n",
    "    warmup_ratio=0.1,\n",
    "    weight_decay=0.01,               # strength of weight decay\n",
    "    logging_steps=50,\n",
    "    learning_rate=2e-5\n",
    ")\n",
    "\n",
    "# Trainer object\n",
    "trainer = Trainer(\n",
    "    model=model,                        # the instantiated ü§ó Transformers model to be trained\n",
    "    args=training_args,                 # training arguments, defined above\n",
    "    train_dataset=processed_train_data, # training dataset\n",
    "    eval_dataset=processed_test_data,   # evaluation dataset\n",
    "    compute_metrics=compute_f1_metrics\n",
    ")\n",
    "\n",
    "# Training loop\n",
    "trainer.train(resume_from_checkpoint=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d7139869",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/l0k00au/Projects/Leoric/AI/leoric.kb.ai.huggingface/.venv/lib/python3.14/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='375' max='375' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [375/375 02:10]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "È¢ÑÊµãÂÄºÁ±ªÂûã: <class 'numpy.ndarray'>, ÂΩ¢Áä∂: (3000, 512)\n",
      "ÁúüÂÆûÊ†áÁ≠æÁ±ªÂûã: <class 'numpy.ndarray'>ÔºåÂΩ¢Áä∂: (3000,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': nan,\n",
       " 'eval_f1_start': 0.04628814728479341,\n",
       " 'eval_f1_end': 0.0393449251920744,\n",
       " 'eval_runtime': 131.7309,\n",
       " 'eval_samples_per_second': 22.774,\n",
       " 'eval_steps_per_second': 2.847,\n",
       " 'epoch': 3.0}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate(processed_test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "63a5c275",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What superheroes were introduced between 1939 and 1941 by Detective Comics and its sister company?\n",
      "Answer: [CLS]\n",
      "\n",
      "Question: What comic book characters were created between 1939 and 1941?\n",
      "Answer: [CLS]\n",
      "\n",
      "Question: What well-known characters were created between 1939 and 1941?\n",
      "Answer: [CLS]\n",
      "\n",
      "Question: What well-known superheroes were introduced between 1939 and 1941 by Detective Comics?\n",
      "Answer: [CLS]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text = r\"\"\"\n",
    "The Golden Age of Comic Books describes an era of American comic books from the\n",
    "late 1930s to circa 1950. During this time, modern comic books were first published\n",
    "and rapidly increased in popularity. The superhero archetype was created and many\n",
    "well-known characters were introduced, including Superman, Batman, Captain Marvel\n",
    "(later known as SHAZAM!), Captain America, and Wonder Woman.\n",
    "Between 1939 and 1941 Detective Comics and its sister company, All-American Publications,\n",
    "introduced popular superheroes such as Batman and Robin, Wonder Woman, the Flash,\n",
    "Green Lantern, Doctor Fate, the Atom, Hawkman, Green Arrow and Aquaman.[7] Timely Comics,\n",
    "the 1940s predecessor of Marvel Comics, had million-selling titles featuring the Human Torch,\n",
    "the Sub-Mariner, and Captain America.[8]\n",
    "As comic books grew in popularity, publishers began launching titles that expanded\n",
    "into a variety of genres. Dell Comics' non-superhero characters (particularly the\n",
    "licensed Walt Disney animated-character comics) outsold the superhero comics of the day.[12]\n",
    "The publisher featured licensed movie and literary characters such as Mickey Mouse, Donald Duck,\n",
    "Roy Rogers and Tarzan.[13] It was during this era that noted Donald Duck writer-artist\n",
    "Carl Barks rose to prominence.[14] Additionally, MLJ's introduction of Archie Andrews\n",
    "in Pep Comics #22 (December 1941) gave rise to teen humor comics,[15] with the Archie\n",
    "Andrews character remaining in print well into the 21st century.[16]\n",
    "At the same time in Canada, American comic books were prohibited importation under\n",
    "the War Exchange Conservation Act[17] which restricted the importation of non-essential\n",
    "goods. As a result, a domestic publishing industry flourished during the duration\n",
    "of the war which were collectively informally called the Canadian Whites.\n",
    "The educational comic book Dagwood Splits the Atom used characters from the comic\n",
    "strip Blondie.[18] According to historian Michael A. Amundson, appealing comic-book\n",
    "characters helped ease young readers' fear of nuclear war and neutralize anxiety\n",
    "about the questions posed by atomic power.[19] It was during this period that long-running\n",
    "humor comics debuted, including EC's Mad and Carl Barks' Uncle Scrooge in Dell's Four\n",
    "Color Comics (both in 1952).[20][21]\n",
    "\"\"\"\n",
    "\n",
    "questions = [\"What superheroes were introduced between 1939 and 1941 by Detective Comics and its sister company?\",\n",
    "             \"What comic book characters were created between 1939 and 1941?\",\n",
    "             \"What well-known characters were created between 1939 and 1941?\",\n",
    "             \"What well-known superheroes were introduced between 1939 and 1941 by Detective Comics?\"]\n",
    "\n",
    "for question in questions:\n",
    "    inputs = tokenizer.encode_plus(question, text, return_tensors=\"pt\") # same as: inputs = tokenizer(question, text, return_tensor=\"pt\")\n",
    "    # inputs: {\"input_ids\": [[101, 1327, ...]], \"attention_mask\": [[1, 1, ...]]}\n",
    "    input_ids = inputs[\"input_ids\"].tolist()[0]\n",
    "    inputs.to(\"mps\") # Use \"cuda\" for GPU or \"mps\" for Apple Silicon\n",
    "\n",
    "    text_tokens = tokenizer.convert_ids_to_tokens(input_ids) # text_tokens: ['[CLS]', 'What', 'superhero', '##es', 'were', ..., '[SEP]', 'The', 'Golden', 'Age', ..., '[SEP]']\n",
    "    answer_model = model(**inputs)\n",
    "    \n",
    "    start_logits = answer_model['start_logits'].cpu().detach().numpy()\n",
    "\n",
    "    answer_start = np.argmax(start_logits)  \n",
    "    \n",
    "    end_logits = answer_model['end_logits'].cpu().detach().numpy()\n",
    "    \n",
    "    # Get the most likely beginning of answer with the argmax of the score\n",
    "    answer_end = np.argmax(end_logits) + 1  # Get the most likely end of answer with the argmax of the score\n",
    "\n",
    "    answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(input_ids[answer_start:answer_end]))\n",
    "\n",
    "    print(f\"Question: {question}\")\n",
    "    print(f\"Answer: {answer}\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "leoric-kb-ai-huggingface (3.14.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
